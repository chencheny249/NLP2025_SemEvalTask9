{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641372c5",
   "metadata": {},
   "source": [
    "# ROBERTA BASE HYPERPARAMETER TUNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2676 samples\n",
      "Dataset memory: 0.60 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "train_raw = pd.read_csv(\"/Users/arwynlewis/Desktop/NLP/NLP2025_SemEvalTask9/dev_phase/subtask1/train/eng.csv\")\n",
    "\n",
    "print(f\"Dataset size: {len(train_raw)} samples\")\n",
    "print(f\"Dataset memory: {train_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "device_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup with fallback to CPU if needed\n",
    "USE_CPU = False  \n",
    "\n",
    "if USE_CPU:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS\")\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4baf6734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param combinations: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# MINIMAL parameter grid \n",
    "param_grid = {\n",
    "    \"learning_rate\": [2e-5, 3e-5],\n",
    "    \"num_train_epochs\": [3],  \n",
    "    \"per_device_train_batch_size\": [1, 2],  \n",
    "    \"gradient_accumulation_steps\": [32]  \n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "print(f\"Total param combinations: {len(grid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df662928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Use 3 folds\n",
    "k = 3\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aafea038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2408\n",
      "Test samples: 268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clean NaNs\n",
    "train_raw = train_raw.dropna(subset=['text', 'polarization']).reset_index(drop=True)\n",
    "\n",
    "# Split train into train/validation\n",
    "train, test = train_test_split(train_raw, test_size=0.1, random_state=42, stratify=train_raw['polarization'])\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22c15e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "        item['labels'] = torch.tensor(int(label), dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda3b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ef5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "memory_cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGRESSIVE memory cleanup\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    gc.collect() \n",
    "    if torch.backends.mps.is_available() and not USE_CPU:\n",
    "        torch.mps.empty_cache()\n",
    "        torch.mps.synchronize()  \n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb9a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing param combination 1/4\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 2e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 1}\n",
      "Effective batch size: 32\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 22:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501132</td>\n",
       "      <td>0.757495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.788673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.442344</td>\n",
       "      <td>0.788531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.7885\n",
      "\n",
      "Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 17:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472039</td>\n",
       "      <td>0.767106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.439284</td>\n",
       "      <td>0.793216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.424486</td>\n",
       "      <td>0.809545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1: 0.8095\n",
      "\n",
      "Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 21:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461409</td>\n",
       "      <td>0.781613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.421445</td>\n",
       "      <td>0.808811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.416828</td>\n",
       "      <td>0.799891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='802' max='802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [802/802 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1: 0.7999\n",
      "\n",
      "================================================================================\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 2e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 1}\n",
      "Mean Macro F1: 0.7993 (+/- 0.0086)\n",
      "Effective batch size: 32\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing param combination 2/4\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 2e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 2}\n",
      "Effective batch size: 64\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 11:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.515254</td>\n",
       "      <td>0.708022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428661</td>\n",
       "      <td>0.791964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422984</td>\n",
       "      <td>0.792364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.7924\n",
      "\n",
      "Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 11:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551985</td>\n",
       "      <td>0.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426386</td>\n",
       "      <td>0.785275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428645</td>\n",
       "      <td>0.797414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1: 0.7974\n",
      "\n",
      "Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 11:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.521209</td>\n",
       "      <td>0.639669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418187</td>\n",
       "      <td>0.805520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.404871</td>\n",
       "      <td>0.806641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='802' max='802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [802/802 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1: 0.8066\n",
      "\n",
      "================================================================================\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 2e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 2}\n",
      "Mean Macro F1: 0.7988 (+/- 0.0059)\n",
      "Effective batch size: 64\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing param combination 3/4\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 3e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 1}\n",
      "Effective batch size: 32\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 21:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438395</td>\n",
       "      <td>0.789269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.429581</td>\n",
       "      <td>0.791846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.476603</td>\n",
       "      <td>0.796383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.7964\n",
      "\n",
      "Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 22:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435801</td>\n",
       "      <td>0.779947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.521014</td>\n",
       "      <td>0.761729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.431992</td>\n",
       "      <td>0.809649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1: 0.8096\n",
      "\n",
      "Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 16:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431749</td>\n",
       "      <td>0.798951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.405508</td>\n",
       "      <td>0.816614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.422359</td>\n",
       "      <td>0.797719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='802' max='802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [802/802 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1: 0.7977\n",
      "\n",
      "================================================================================\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 3e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 1}\n",
      "Mean Macro F1: 0.8013 (+/- 0.0060)\n",
      "Effective batch size: 32\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing param combination 4/4\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 3e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 2}\n",
      "Effective batch size: 64\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 09:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484357</td>\n",
       "      <td>0.757760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.420975</td>\n",
       "      <td>0.794119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434525</td>\n",
       "      <td>0.799346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.7993\n",
      "\n",
      "Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 09:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.494581</td>\n",
       "      <td>0.754347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474881</td>\n",
       "      <td>0.783556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431234</td>\n",
       "      <td>0.807921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='803' max='803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [803/803 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1: 0.8079\n",
      "\n",
      "Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 08:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457599</td>\n",
       "      <td>0.783744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427116</td>\n",
       "      <td>0.784361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407151</td>\n",
       "      <td>0.811211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='802' max='802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [802/802 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1: 0.8112\n",
      "\n",
      "================================================================================\n",
      "Params: {'gradient_accumulation_steps': 32, 'learning_rate': 3e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 2}\n",
      "Mean Macro F1: 0.8062 (+/- 0.0050)\n",
      "Effective batch size: 64\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for param_idx, params in enumerate(grid):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing param combination {param_idx + 1}/{len(grid)}\")\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"Effective batch size: {params['per_device_train_batch_size'] * params['gradient_accumulation_steps']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    f1_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['polarization'])):\n",
    "        print(f\"\\nFold {fold_idx + 1}/{k}\")\n",
    "        \n",
    "        # Clean memory\n",
    "        cleanup_memory()\n",
    "        \n",
    "        # Split data\n",
    "        train_fold = train.iloc[train_idx].reset_index(drop=True)\n",
    "        val_fold = train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = PolarizationDataset(\n",
    "            train_fold['text'].tolist(),\n",
    "            train_fold['polarization'].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "        val_dataset = PolarizationDataset(\n",
    "            val_fold['text'].tolist(),\n",
    "            val_fold['polarization'].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        # Re-initialize model for each fold\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "        \n",
    "        # Enable gradient checkpointing (memory saver)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Move model to device\n",
    "        if not USE_CPU:\n",
    "            model = model.to(device)\n",
    "\n",
    "        # Training arguments \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./roberta_output\",\n",
    "            num_train_epochs=params['num_train_epochs'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            per_device_train_batch_size=params['per_device_train_batch_size'],\n",
    "            per_device_eval_batch_size=1,  # MINIMUM - batch size 1 for evaluation\n",
    "            gradient_accumulation_steps=params['gradient_accumulation_steps'],\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_steps=100,\n",
    "            disable_tqdm=False,\n",
    "            report_to=[],\n",
    "            fp16=False, \n",
    "            dataloader_num_workers=0,\n",
    "            load_best_model_at_end=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            max_grad_norm=1.0,\n",
    "            # Additional memory optimizations\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch\",  \n",
    "            save_total_limit=0,  \n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "            f1_scores.append(eval_results['eval_f1_macro'])\n",
    "            \n",
    "            print(f\"Fold {fold_idx + 1} F1: {eval_results['eval_f1_macro']:.4f}\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"\\n{'!'*80}\")\n",
    "                print(\"MEMORY ERROR DETECTED!\")\n",
    "                print(f\"{'!'*80}\")\n",
    "                print(\"\\nOptions:\")\n",
    "                print(\"1. Set USE_CPU = True in the device setup cell and rerun\")\n",
    "                print(\"2. Close other applications to free up memory\")\n",
    "                print(\"3. Consider using a smaller model like DistilRoBERTa\")\n",
    "                print(f\"\\n{'!'*80}\\n\")\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Clean up after each fold\n",
    "        del model, trainer, train_dataset, val_dataset\n",
    "        cleanup_memory()\n",
    "\n",
    "    if f1_scores: \n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        std_f1 = np.std(f1_scores)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Params: {params}\")\n",
    "        print(f\"Mean Macro F1: {mean_f1:.4f} (+/- {std_f1:.4f})\")\n",
    "        print(f\"Effective batch size: {params['per_device_train_batch_size'] * params['gradient_accumulation_steps']}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        all_results.append({\n",
    "            'params': params,\n",
    "            'mean_f1': mean_f1,\n",
    "            'std_f1': std_f1,\n",
    "            'fold_scores': f1_scores\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "show_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Results (sorted by Mean F1):\n",
      " learning_rate  epochs  batch_size  grad_accum  effective_batch  mean_f1   std_f1\n",
      "       0.00003       3           2          32               64 0.806159 0.005001\n",
      "       0.00003       3           1          32               32 0.801250 0.005963\n",
      "       0.00002       3           1          32               32 0.799322 0.008588\n",
      "       0.00002       3           2          32               64 0.798806 0.005911\n",
      "\n",
      "================================================================================\n",
      "BEST PARAMETERS:\n",
      "Parameters: {'gradient_accumulation_steps': 32, 'learning_rate': 3e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 2}\n",
      "Mean Macro F1: 0.8062 (+/- 0.0050)\n",
      "Fold scores: ['0.7993', '0.8079', '0.8112']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display all results sorted by mean F1\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'learning_rate': r['params']['learning_rate'],\n",
    "            'epochs': r['params']['num_train_epochs'],\n",
    "            'batch_size': r['params']['per_device_train_batch_size'],\n",
    "            'grad_accum': r['params']['gradient_accumulation_steps'],\n",
    "            'effective_batch': r['params']['per_device_train_batch_size'] * r['params']['gradient_accumulation_steps'],\n",
    "            'mean_f1': r['mean_f1'],\n",
    "            'std_f1': r['std_f1']\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "\n",
    "    results_df = results_df.sort_values('mean_f1', ascending=False)\n",
    "    print(\"\\nAll Results (sorted by Mean F1):\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST PARAMETERS:\")\n",
    "    best_result = max(all_results, key=lambda x: x['mean_f1'])\n",
    "    print(f\"Parameters: {best_result['params']}\")\n",
    "    print(f\"Mean Macro F1: {best_result['mean_f1']:.4f} (+/- {best_result['std_f1']:.4f})\")\n",
    "    print(f\"Fold scores: {[f'{score:.4f}' for score in best_result['fold_scores']]}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No results available. Check if training completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67459e6",
   "metadata": {},
   "source": [
    "# Train Final Model on Full Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4ea3caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "MPS available: True\n",
      "CUDA available: False\n",
      "Using device: cpu\n",
      "Training on 2676 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/DTSC-5501/lib/python3.11/site-packages/transformers/training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Total steps: 249\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 09:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import gc\n",
    "\n",
    "# FORCE CPU USAGE - Set this BEFORE any model loading\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  \n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'  \n",
    "\n",
    "# Verify CPU is being used\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Force CPU device\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear any GPU memory\n",
    "gc.collect()\n",
    "\n",
    "# Load your full training data\n",
    "train_raw = pd.read_csv(\"/Users/arwynlewis/Desktop/NLP/NLP2025_SemEvalTask9/dev_phase/subtask1/train/eng.csv\")\n",
    "train_raw = train_raw.dropna(subset=['text', 'polarization']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training on {len(train_raw)} samples\")\n",
    "\n",
    "# Define Dataset class\n",
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "        item['labels'] = torch.tensor(int(label), dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create dataset with ALL training data\n",
    "full_dataset = PolarizationDataset(\n",
    "    train_raw['text'].tolist(),\n",
    "    train_raw['polarization'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Initialize final model \n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', \n",
    "    num_labels=2,\n",
    "    device_map='cpu',  \n",
    "    torch_dtype=torch.float32  \n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "final_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments with BEST hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final_roberta_model\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,  \n",
    "    gradient_accumulation_steps=8,  \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_total_limit=1,\n",
    "    no_cuda=True, \n",
    "    use_cpu=True,  \n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Total steps: {len(full_dataset) // 4 // 8 * 3}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./final_roberta_polarization\")\n",
    "tokenizer.save_pretrained(\"./final_roberta_polarization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9a7969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 133\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions generated successfully (no labels to evaluate)\n",
      "\n",
      "Predictions saved to 'submission_roberta.csv'\n",
      "\n",
      "Prediction Distribution:\n",
      "   Non-Polarized (0):    90 ( 67.7%)\n",
      "   Polarized (1):        43 ( 32.3%)\n",
      "\n",
      " Average Confidence: 0.888\n",
      "\n",
      " Sample Predictions:\n",
      "====================================================================================================\n",
      "ID     Pred         Conf     Text\n",
      "====================================================================================================\n",
      "eng_f66ca14d60851371f9720aaf4ccd9b58 Non-Polar    0.927    God is with Ukraine and Zelensky\n",
      "eng_3a489aa7fed9726aa8d3d4fe74c57efb Non-Polar    0.995    4 Dems, 2 Republicans Luzerne County Council seatsDallas\n",
      "eng_95770ff547ea5e48b0be00f385986483 Non-Polar    0.994    Abuse Survivor Recounts Her Struggles at YWCA Event\n",
      "eng_2048ae6f9aa261c48e6d777bcc5b38bf Non-Polar    0.609    After Rwanda, another deportation camp disaster\n",
      "eng_07781aa88e61e7c0a996abd1e5ea3a20 Non-Polar    0.991    Another plea in Trump election interference probe\n",
      "eng_153d96f9dc27f0602c927223404d94b5 Non-Polar    0.837    any number of southern red states tbh\n",
      "eng_4ab5a4cc5c87d0af9cf4b80c301647bf Non-Polar    0.629    Breitbart is the new age grocery aisle tabloid\n",
      "eng_e75a95ba52930d6d72d503ab9469eb29 Non-Polar    0.995    Congressman talks border security, debit limit\n",
      "eng_eb8fab668668f9959cafdecbfc0f081a Non-Polar    0.995    Cooke Co. GOP incumbents announce reelection runs\n",
      "eng_702724dc168d600e788d775c8e651f36 Non-Polar    0.926    Could one overwhelm the iron dome?\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv(\"/Users/arwynlewis/Desktop/NLP/NLP2025_SemEvalTask9/dev_phase/subtask1/dev/eng.csv\")\n",
    "\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Check for labels and handle NaN values\n",
    "if 'polarization' in test_data.columns:\n",
    "    # Check lables\n",
    "    if test_data['polarization'].notna().any():\n",
    "        test_data_clean = test_data.dropna(subset=['polarization']).reset_index(drop=True)\n",
    "        test_labels = test_data_clean['polarization'].tolist()\n",
    "        has_labels = True\n",
    "        print(f\" Found {len(test_data_clean)} samples with labels\")\n",
    "        if len(test_data_clean) < len(test_data):\n",
    "            print(f\"Removed {len(test_data) - len(test_data_clean)} samples with missing labels\")\n",
    "        test_data = test_data_clean \n",
    "    else:\n",
    "        # All labels are NaN \n",
    "        test_labels = [0] * len(test_data)\n",
    "        has_labels = False\n",
    "else:\n",
    "    test_labels = [0] * len(test_data)\n",
    "    has_labels = False\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = PolarizationDataset(\n",
    "    test_data['text'].tolist(),\n",
    "    test_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "prediction_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Calculate metrics if labels available\n",
    "if has_labels:\n",
    "    f1 = f1_score(test_data['polarization'], predicted_labels, average='macro')\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Set Macro F1: {f1:.4f}\")\n",
    "    print(f\"Expected range based on CV: 0.807 - 0.812\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(classification_report(\n",
    "        test_data['polarization'], \n",
    "        predicted_labels,\n",
    "        target_names=['Non-Polarized (0)', 'Polarized (1)'],\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Show comparison with CV results\n",
    "    cv_f1 = 0.8095\n",
    "    difference = f1 - cv_f1\n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"   Cross-validation F1: {cv_f1:.4f}\")\n",
    "    print(f\"   Test set F1:         {f1:.4f}\")\n",
    "    print(f\"   Difference:          {difference:+.4f} ({difference/cv_f1*100:+.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nPredictions generated successfully (no labels to evaluate)\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'] if 'id' in test_data.columns else test_data.index,\n",
    "    'polarization': predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_roberta.csv', index=False)\n",
    "print(\"\\nPredictions saved to 'submission_roberta.csv'\")\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "print(f\"   Non-Polarized (0): {(predicted_labels == 0).sum():>5} ({(predicted_labels == 0).sum()/len(predicted_labels)*100:>5.1f}%)\")\n",
    "print(f\"   Polarized (1):     {(predicted_labels == 1).sum():>5} ({(predicted_labels == 1).sum()/len(predicted_labels)*100:>5.1f}%)\")\n",
    "print(f\"\\n Average Confidence: {prediction_probs.max(axis=1).mean():.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n Sample Predictions:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'ID':<6} {'Pred':<12} {'Conf':<8} {'Text'}\")\n",
    "print(\"=\"*100)\n",
    "for i in range(min(10, len(test_data))):\n",
    "    pred_label = \"Polarized\" if predicted_labels[i] == 1 else \"Non-Polar\"\n",
    "    conf = prediction_probs[i].max()\n",
    "    text = test_data['text'].iloc[i]\n",
    "    text_short = text[:60] + \"...\" if len(text) > 60 else text\n",
    "    row_id = test_data['id'].iloc[i] if 'id' in test_data.columns else i\n",
    "    print(f\"{row_id:<6} {pred_label:<12} {conf:<8.3f} {text_short}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c62a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTSC-5501",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
